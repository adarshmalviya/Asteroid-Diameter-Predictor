{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asteroides Diameter Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50) # To see all the columns of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>neo</th>\n",
       "      <th>pha</th>\n",
       "      <th>moid</th>\n",
       "      <th>class</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.59478</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.665712</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.23324</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.03454</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.09589</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         a         e          i          om           w  \\\n",
       "0           0  2.769165  0.076009  10.594067   80.305532   73.597694   \n",
       "1           1  2.772466  0.230337  26.665712  173.080063  310.048857   \n",
       "2           2  2.669150  0.256942  12.988919  169.852760  248.138626   \n",
       "3           3  2.361418  0.088721   7.141771  103.810804  150.728541   \n",
       "4           4  2.574249  0.191095   5.366988  141.576605  358.687607   \n",
       "\n",
       "          q        ad     per_y  data_arc  condition_code  n_obs_used      H  \\\n",
       "0  2.558684  2.979647  4.608202    8822.0               0      1002.0  11.85   \n",
       "1  2.133865  3.411067  4.616444   14881.5               0      2137.5  11.85   \n",
       "2  1.983332  3.354967  4.360814   14881.5               0      2137.5  11.85   \n",
       "3  2.151909  2.570926  3.628837   14881.5               0      2137.5  11.85   \n",
       "4  2.082324  3.066174  4.130323   14881.5               0      2137.5  11.85   \n",
       "\n",
       "  neo pha     moid class         n          per          ma  Diameter  \n",
       "0   N   N  1.59478   MBA  0.213885  1683.145708   77.372096      10.2  \n",
       "1   N   N  1.23324   MBA  0.213503  1686.155999   59.699133      10.2  \n",
       "2   N   N  1.03454   MBA  0.226019  1592.787285   34.925016      10.2  \n",
       "3   N   N  1.13948   MBA  0.271609  1325.432765   95.861936      10.2  \n",
       "4   N   N  1.09589   MBA  0.238632  1508.600458  282.366289      10.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Clean_Dataset.csv')  # To Read dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>neo</th>\n",
       "      <th>pha</th>\n",
       "      <th>moid</th>\n",
       "      <th>class</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.59478</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.665712</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.23324</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.03454</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.09589</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         e          i          om           w         q        ad  \\\n",
       "0  2.769165  0.076009  10.594067   80.305532   73.597694  2.558684  2.979647   \n",
       "1  2.772466  0.230337  26.665712  173.080063  310.048857  2.133865  3.411067   \n",
       "2  2.669150  0.256942  12.988919  169.852760  248.138626  1.983332  3.354967   \n",
       "3  2.361418  0.088721   7.141771  103.810804  150.728541  2.151909  2.570926   \n",
       "4  2.574249  0.191095   5.366988  141.576605  358.687607  2.082324  3.066174   \n",
       "\n",
       "      per_y  data_arc  condition_code  n_obs_used      H neo pha     moid  \\\n",
       "0  4.608202    8822.0               0      1002.0  11.85   N   N  1.59478   \n",
       "1  4.616444   14881.5               0      2137.5  11.85   N   N  1.23324   \n",
       "2  4.360814   14881.5               0      2137.5  11.85   N   N  1.03454   \n",
       "3  3.628837   14881.5               0      2137.5  11.85   N   N  1.13948   \n",
       "4  4.130323   14881.5               0      2137.5  11.85   N   N  1.09589   \n",
       "\n",
       "  class         n          per          ma  Diameter  \n",
       "0   MBA  0.213885  1683.145708   77.372096      10.2  \n",
       "1   MBA  0.213503  1686.155999   59.699133      10.2  \n",
       "2   MBA  0.226019  1592.787285   34.925016      10.2  \n",
       "3   MBA  0.271609  1325.432765   95.861936      10.2  \n",
       "4   MBA  0.238632  1508.600458  282.366289      10.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Data :  ['neo', 'pha', 'class']\n",
      "Numeric Data :  ['a', 'e', 'i', 'om', 'w', 'q', 'ad', 'per_y', 'data_arc', 'condition_code', 'n_obs_used', 'H', 'moid', 'n', 'per', 'ma', 'Diameter']\n"
     ]
    }
   ],
   "source": [
    "# Segregate Data into numeric and categorical onces\n",
    "categorical, numeric = [], []\n",
    "for ele in dataset.columns:\n",
    "    if dataset[ele].dtype == 'object':\n",
    "        categorical.append(ele)\n",
    "    else:\n",
    "        numeric.append(ele)\n",
    "print(\"Categorical Data : \", categorical)\n",
    "print(\"Numeric Data : \", numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>neo</th>\n",
       "      <th>pha</th>\n",
       "      <th>moid</th>\n",
       "      <th>class</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.59478</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.665712</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.23324</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.03454</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.09589</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         e          i          om           w         q        ad  \\\n",
       "0  2.769165  0.076009  10.594067   80.305532   73.597694  2.558684  2.979647   \n",
       "1  2.772466  0.230337  26.665712  173.080063  310.048857  2.133865  3.411067   \n",
       "2  2.669150  0.256942  12.988919  169.852760  248.138626  1.983332  3.354967   \n",
       "3  2.361418  0.088721   7.141771  103.810804  150.728541  2.151909  2.570926   \n",
       "4  2.574249  0.191095   5.366988  141.576605  358.687607  2.082324  3.066174   \n",
       "\n",
       "      per_y  data_arc  condition_code  n_obs_used      H neo pha     moid  \\\n",
       "0  4.608202    8822.0               0      1002.0  11.85   N   N  1.59478   \n",
       "1  4.616444   14881.5               0      2137.5  11.85   N   N  1.23324   \n",
       "2  4.360814   14881.5               0      2137.5  11.85   N   N  1.03454   \n",
       "3  3.628837   14881.5               0      2137.5  11.85   N   N  1.13948   \n",
       "4  4.130323   14881.5               0      2137.5  11.85   N   N  1.09589   \n",
       "\n",
       "  class         n          per          ma  Diameter  \n",
       "0   MBA  0.213885  1683.145708   77.372096      10.2  \n",
       "1   MBA  0.213503  1686.155999   59.699133      10.2  \n",
       "2   MBA  0.226019  1592.787285   34.925016      10.2  \n",
       "3   MBA  0.271609  1325.432765   95.861936      10.2  \n",
       "4   MBA  0.238632  1508.600458  282.366289      10.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categorical Data into Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Data :  ['neo', 'pha', 'class']\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical Data : \",categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neo_dummy = pd.get_dummies(dataset['neo'], drop_first=True)\n",
    "neo_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pha_dummy = pd.get_dummies(dataset['pha'], drop_first=True)\n",
    "pha_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APO</th>\n",
       "      <th>AST</th>\n",
       "      <th>ATE</th>\n",
       "      <th>CEN</th>\n",
       "      <th>IMB</th>\n",
       "      <th>MBA</th>\n",
       "      <th>MCA</th>\n",
       "      <th>OMB</th>\n",
       "      <th>TJN</th>\n",
       "      <th>TNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   APO  AST  ATE  CEN  IMB  MBA  MCA  OMB  TJN  TNO\n",
       "0    0    0    0    0    0    1    0    0    0    0\n",
       "1    0    0    0    0    0    1    0    0    0    0\n",
       "2    0    0    0    0    0    1    0    0    0    0\n",
       "3    0    0    0    0    0    1    0    0    0    0\n",
       "4    0    0    0    0    0    1    0    0    0    0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dummy = pd.get_dummies(dataset['class'], drop_first=True)\n",
    "class_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>condition_code</th>\n",
       "      <th>n_obs_used</th>\n",
       "      <th>H</th>\n",
       "      <th>moid</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Y</th>\n",
       "      <th>Y</th>\n",
       "      <th>APO</th>\n",
       "      <th>AST</th>\n",
       "      <th>ATE</th>\n",
       "      <th>CEN</th>\n",
       "      <th>IMB</th>\n",
       "      <th>MBA</th>\n",
       "      <th>MCA</th>\n",
       "      <th>OMB</th>\n",
       "      <th>TJN</th>\n",
       "      <th>TNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.769165</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>10.594067</td>\n",
       "      <td>80.305532</td>\n",
       "      <td>73.597694</td>\n",
       "      <td>2.558684</td>\n",
       "      <td>2.979647</td>\n",
       "      <td>4.608202</td>\n",
       "      <td>8822.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>11.85</td>\n",
       "      <td>1.59478</td>\n",
       "      <td>0.213885</td>\n",
       "      <td>1683.145708</td>\n",
       "      <td>77.372096</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.772466</td>\n",
       "      <td>0.230337</td>\n",
       "      <td>26.665712</td>\n",
       "      <td>173.080063</td>\n",
       "      <td>310.048857</td>\n",
       "      <td>2.133865</td>\n",
       "      <td>3.411067</td>\n",
       "      <td>4.616444</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>1.23324</td>\n",
       "      <td>0.213503</td>\n",
       "      <td>1686.155999</td>\n",
       "      <td>59.699133</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.669150</td>\n",
       "      <td>0.256942</td>\n",
       "      <td>12.988919</td>\n",
       "      <td>169.852760</td>\n",
       "      <td>248.138626</td>\n",
       "      <td>1.983332</td>\n",
       "      <td>3.354967</td>\n",
       "      <td>4.360814</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>1.03454</td>\n",
       "      <td>0.226019</td>\n",
       "      <td>1592.787285</td>\n",
       "      <td>34.925016</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361418</td>\n",
       "      <td>0.088721</td>\n",
       "      <td>7.141771</td>\n",
       "      <td>103.810804</td>\n",
       "      <td>150.728541</td>\n",
       "      <td>2.151909</td>\n",
       "      <td>2.570926</td>\n",
       "      <td>3.628837</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>1.13948</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>1325.432765</td>\n",
       "      <td>95.861936</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.574249</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>5.366988</td>\n",
       "      <td>141.576605</td>\n",
       "      <td>358.687607</td>\n",
       "      <td>2.082324</td>\n",
       "      <td>3.066174</td>\n",
       "      <td>4.130323</td>\n",
       "      <td>14881.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2137.5</td>\n",
       "      <td>11.85</td>\n",
       "      <td>1.09589</td>\n",
       "      <td>0.238632</td>\n",
       "      <td>1508.600458</td>\n",
       "      <td>282.366289</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         e          i          om           w         q        ad  \\\n",
       "0  2.769165  0.076009  10.594067   80.305532   73.597694  2.558684  2.979647   \n",
       "1  2.772466  0.230337  26.665712  173.080063  310.048857  2.133865  3.411067   \n",
       "2  2.669150  0.256942  12.988919  169.852760  248.138626  1.983332  3.354967   \n",
       "3  2.361418  0.088721   7.141771  103.810804  150.728541  2.151909  2.570926   \n",
       "4  2.574249  0.191095   5.366988  141.576605  358.687607  2.082324  3.066174   \n",
       "\n",
       "      per_y  data_arc  condition_code  n_obs_used      H     moid         n  \\\n",
       "0  4.608202    8822.0               0      1002.0  11.85  1.59478  0.213885   \n",
       "1  4.616444   14881.5               0      2137.5  11.85  1.23324  0.213503   \n",
       "2  4.360814   14881.5               0      2137.5  11.85  1.03454  0.226019   \n",
       "3  3.628837   14881.5               0      2137.5  11.85  1.13948  0.271609   \n",
       "4  4.130323   14881.5               0      2137.5  11.85  1.09589  0.238632   \n",
       "\n",
       "           per          ma  Diameter  Y  Y  APO  AST  ATE  CEN  IMB  MBA  MCA  \\\n",
       "0  1683.145708   77.372096      10.2  0  0    0    0    0    0    0    1    0   \n",
       "1  1686.155999   59.699133      10.2  0  0    0    0    0    0    0    1    0   \n",
       "2  1592.787285   34.925016      10.2  0  0    0    0    0    0    0    1    0   \n",
       "3  1325.432765   95.861936      10.2  0  0    0    0    0    0    0    1    0   \n",
       "4  1508.600458  282.366289      10.2  0  0    0    0    0    0    0    1    0   \n",
       "\n",
       "   OMB  TJN  TNO  \n",
       "0    0    0    0  \n",
       "1    0    0    0  \n",
       "2    0    0    0  \n",
       "3    0    0    0  \n",
       "4    0    0    0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.concat([dataset, neo_dummy, pha_dummy, class_dummy], axis=1)\n",
    "new_data.drop(['neo', 'pha', 'class'], axis=1, inplace=True)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data.drop('Diameter', axis=1), new_data['Diameter'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_scale = StandardScaler()\n",
    "X_train = sc_scale.fit_transform(X_train)\n",
    "X_test = sc_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y_actual):\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "\n",
    "    print(\"Mean Absolute Error :-> \", mae)\n",
    "    print(\"Mean Squared Error :-> \", mse)\n",
    "    print(\"Root Mean Squared Error :-> \", rmse)\n",
    "    print(\"R-Square :-> \", r2)\n",
    "\n",
    "    return mae, mse, rmse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score = {}     # For Storing Algoriths name and its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :->  0.6846960296788728\n",
      "Mean Squared Error :->  0.9340603938280014\n",
      "Root Mean Squared Error :->  0.9664679993812529\n",
      "R-Square :->  0.8104701460640636\n"
     ]
    }
   ],
   "source": [
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Random Forest'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbour Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=4)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :->  0.7672460585585585\n",
      "Mean Squared Error :->  1.2024815882646758\n",
      "Root Mean Squared Error :->  1.0965772149122357\n",
      "R-Square :->  0.7538540204577808\n"
     ]
    }
   ],
   "source": [
    "y_pred_knn = knn.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['K Nearest Neighbour'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_r = LinearRegression()\n",
    "linear_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :->  1.0351879503976906\n",
      "Mean Squared Error :->  1.581760120453736\n",
      "Root Mean Squared Error :->  1.2576804524416112\n",
      "R-Square :->  0.6358846067618822\n"
     ]
    }
   ],
   "source": [
    "y_pred_linear = linear_r.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Linear Regression'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :->  0.9156096156640554\n",
      "Mean Squared Error :->  1.917691502261334\n",
      "Root Mean Squared Error :->  1.384807388145129\n",
      "R-Square :->  0.6750111802932943\n"
     ]
    }
   ],
   "source": [
    "y_pred_tree = tree.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Decision Tree'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_r = XGBRegressor()\n",
    "xgb_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :->  0.6988885295922876\n",
      "Mean Squared Error :->  0.947227624108515\n",
      "Root Mean Squared Error :->  0.9732561965425728\n",
      "R-Square :->  0.8101861170775692\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = xgb_r.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['XG Boost'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining HyperParameters\n",
    "hyper_param = {'learning_rate':[0.290,0.30,0.301],\n",
    "    'max_depth':[4,6,8],\n",
    "    'min_child_weight':[1,3],\n",
    "    'gamma':[0,0.1,0.2],\n",
    "    'colsample_bytree':[0.9,1,1.1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using RandomizedSearchCV for finding the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(xgb_r,param_distributions=hyper_param,n_jobs=-1, scoring=\"neg_mean_squared_error\",cv=4, verbose=3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4,\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          gpu_id=-1, importance_type='gain',\n",
       "                                          interaction_constraints='',\n",
       "                                          learning_rate=0.300000012,\n",
       "                                          max_delta_step=0, max_depth=6,\n",
       "                                          min_child_weight=1, missing=nan,\n",
       "                                          monotone_constraints='()',\n",
       "                                          n_estimators=100, n_jobs=8,\n",
       "                                          num_par..., random_state=0,\n",
       "                                          reg_alpha=0, reg_lambda=1,\n",
       "                                          scale_pos_weight=1, subsample=1,\n",
       "                                          tree_method='exact',\n",
       "                                          validate_parameters=1,\n",
       "                                          verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.9, 1, 1.1],\n",
       "                                        'gamma': [0, 0.1, 0.2],\n",
       "                                        'learning_rate': [0.29, 0.3, 0.301],\n",
       "                                        'max_depth': [4, 6, 8],\n",
       "                                        'min_child_weight': [1, 3]},\n",
       "                   random_state=10, scoring='neg_mean_squared_error',\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters found are : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 1,\n",
       " 'max_depth': 8,\n",
       " 'learning_rate': 0.29,\n",
       " 'gamma': 0,\n",
       " 'colsample_bytree': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Parameters found are : \")\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :->  0.6877365244554777\n",
      "Mean Squared Error :->  0.9396361078429152\n",
      "Root Mean Squared Error :->  0.9693482902666695\n",
      "R-Square :->  0.8140431669907069\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb_opt = random_search.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_xgb_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['Optimized XG Boost'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(input_dim=X_train.shape[1], units=12, kernel_initializer=\"he_uniform\", activation=\"relu\"))\n",
    "ann.add(Dense(units=10, kernel_initializer=\"he_uniform\", activation=\"relu\"))\n",
    "ann.add(Dense(units=1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                348       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                130       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 489\n",
      "Trainable params: 489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 3.5273 - val_loss: 1.6300\n",
      "Epoch 2/200\n",
      "771/771 [==============================] - 1s 2ms/step - loss: 1.4480 - val_loss: 1.3558\n",
      "Epoch 3/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.3224 - val_loss: 1.2933\n",
      "Epoch 4/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.2816 - val_loss: 1.2663\n",
      "Epoch 5/200\n",
      "771/771 [==============================] - 1s 2ms/step - loss: 1.2596 - val_loss: 1.2466\n",
      "Epoch 6/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.2416 - val_loss: 1.2357\n",
      "Epoch 7/200\n",
      "771/771 [==============================] - 1s 942us/step - loss: 1.2283 - val_loss: 1.2262\n",
      "Epoch 8/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.2201 - val_loss: 1.2103\n",
      "Epoch 9/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.2030 - val_loss: 1.1941\n",
      "Epoch 10/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1911 - val_loss: 1.1804\n",
      "Epoch 11/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1803 - val_loss: 1.1776\n",
      "Epoch 12/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1648 - val_loss: 1.1576\n",
      "Epoch 13/200\n",
      "771/771 [==============================] - 1s 914us/step - loss: 1.1516 - val_loss: 1.1572\n",
      "Epoch 14/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1424 - val_loss: 1.1452\n",
      "Epoch 15/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1349 - val_loss: 1.1304\n",
      "Epoch 16/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1309 - val_loss: 1.1449\n",
      "Epoch 17/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1290 - val_loss: 1.1280\n",
      "Epoch 18/200\n",
      "771/771 [==============================] - 1s 912us/step - loss: 1.1232 - val_loss: 1.1198\n",
      "Epoch 19/200\n",
      "771/771 [==============================] - 1s 884us/step - loss: 1.1244 - val_loss: 1.1162\n",
      "Epoch 20/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1210 - val_loss: 1.1230\n",
      "Epoch 21/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1169 - val_loss: 1.1177\n",
      "Epoch 22/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.1159 - val_loss: 1.1167\n",
      "Epoch 23/200\n",
      "771/771 [==============================] - 1s 818us/step - loss: 1.1154 - val_loss: 1.1106\n",
      "Epoch 24/200\n",
      "771/771 [==============================] - 1s 832us/step - loss: 1.1152 - val_loss: 1.1131\n",
      "Epoch 25/200\n",
      "771/771 [==============================] - 1s 855us/step - loss: 1.1141 - val_loss: 1.1078\n",
      "Epoch 26/200\n",
      "771/771 [==============================] - 1s 827us/step - loss: 1.1101 - val_loss: 1.1122\n",
      "Epoch 27/200\n",
      "771/771 [==============================] - 1s 819us/step - loss: 1.1092 - val_loss: 1.1043\n",
      "Epoch 28/200\n",
      "771/771 [==============================] - 1s 853us/step - loss: 1.1103 - val_loss: 1.1051\n",
      "Epoch 29/200\n",
      "771/771 [==============================] - 1s 827us/step - loss: 1.1088 - val_loss: 1.1105\n",
      "Epoch 30/200\n",
      "771/771 [==============================] - 1s 892us/step - loss: 1.1067 - val_loss: 1.1076\n",
      "Epoch 31/200\n",
      "771/771 [==============================] - 1s 832us/step - loss: 1.1074 - val_loss: 1.1017\n",
      "Epoch 32/200\n",
      "771/771 [==============================] - 1s 809us/step - loss: 1.1068 - val_loss: 1.1021\n",
      "Epoch 33/200\n",
      "771/771 [==============================] - 1s 803us/step - loss: 1.1033 - val_loss: 1.1099\n",
      "Epoch 34/200\n",
      "771/771 [==============================] - 1s 829us/step - loss: 1.1034 - val_loss: 1.1018\n",
      "Epoch 35/200\n",
      "771/771 [==============================] - 1s 818us/step - loss: 1.1018 - val_loss: 1.1115\n",
      "Epoch 36/200\n",
      "771/771 [==============================] - 1s 835us/step - loss: 1.1047 - val_loss: 1.1125\n",
      "Epoch 37/200\n",
      "771/771 [==============================] - 1s 835us/step - loss: 1.1016 - val_loss: 1.1034\n",
      "Epoch 38/200\n",
      "771/771 [==============================] - 1s 832us/step - loss: 1.0998 - val_loss: 1.0949\n",
      "Epoch 39/200\n",
      "771/771 [==============================] - 1s 807us/step - loss: 1.0988 - val_loss: 1.0991\n",
      "Epoch 40/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0970 - val_loss: 1.0937\n",
      "Epoch 41/200\n",
      "771/771 [==============================] - 1s 816us/step - loss: 1.0997 - val_loss: 1.1029\n",
      "Epoch 42/200\n",
      "771/771 [==============================] - 1s 827us/step - loss: 1.0978 - val_loss: 1.0938\n",
      "Epoch 43/200\n",
      "771/771 [==============================] - 1s 849us/step - loss: 1.0955 - val_loss: 1.1002\n",
      "Epoch 44/200\n",
      "771/771 [==============================] - 1s 853us/step - loss: 1.0959 - val_loss: 1.1005\n",
      "Epoch 45/200\n",
      "771/771 [==============================] - 1s 822us/step - loss: 1.0944 - val_loss: 1.0929\n",
      "Epoch 46/200\n",
      "771/771 [==============================] - 1s 806us/step - loss: 1.0935 - val_loss: 1.0947\n",
      "Epoch 47/200\n",
      "771/771 [==============================] - 1s 811us/step - loss: 1.0962 - val_loss: 1.0985\n",
      "Epoch 48/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0937 - val_loss: 1.0906\n",
      "Epoch 49/200\n",
      "771/771 [==============================] - 1s 812us/step - loss: 1.0934 - val_loss: 1.0936\n",
      "Epoch 50/200\n",
      "771/771 [==============================] - 1s 820us/step - loss: 1.0926 - val_loss: 1.0917\n",
      "Epoch 51/200\n",
      "771/771 [==============================] - 1s 822us/step - loss: 1.0911 - val_loss: 1.0915\n",
      "Epoch 52/200\n",
      "771/771 [==============================] - 1s 815us/step - loss: 1.0916 - val_loss: 1.0935\n",
      "Epoch 53/200\n",
      "771/771 [==============================] - 1s 821us/step - loss: 1.0914 - val_loss: 1.0899\n",
      "Epoch 54/200\n",
      "771/771 [==============================] - 1s 839us/step - loss: 1.0911 - val_loss: 1.0893\n",
      "Epoch 55/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0894 - val_loss: 1.0950\n",
      "Epoch 56/200\n",
      "771/771 [==============================] - 1s 824us/step - loss: 1.0897 - val_loss: 1.0876\n",
      "Epoch 57/200\n",
      "771/771 [==============================] - 1s 896us/step - loss: 1.0913 - val_loss: 1.0856\n",
      "Epoch 58/200\n",
      "771/771 [==============================] - 1s 791us/step - loss: 1.0904 - val_loss: 1.0950\n",
      "Epoch 59/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0883 - val_loss: 1.0846\n",
      "Epoch 60/200\n",
      "771/771 [==============================] - 1s 826us/step - loss: 1.0892 - val_loss: 1.0954\n",
      "Epoch 61/200\n",
      "771/771 [==============================] - 1s 805us/step - loss: 1.0884 - val_loss: 1.0867\n",
      "Epoch 62/200\n",
      "771/771 [==============================] - 1s 818us/step - loss: 1.0890 - val_loss: 1.0919\n",
      "Epoch 63/200\n",
      "771/771 [==============================] - 1s 829us/step - loss: 1.0875 - val_loss: 1.0872\n",
      "Epoch 64/200\n",
      "771/771 [==============================] - 1s 803us/step - loss: 1.0862 - val_loss: 1.0871\n",
      "Epoch 65/200\n",
      "771/771 [==============================] - 1s 831us/step - loss: 1.0884 - val_loss: 1.0928\n",
      "Epoch 66/200\n",
      "771/771 [==============================] - 1s 820us/step - loss: 1.0866 - val_loss: 1.0880\n",
      "Epoch 67/200\n",
      "771/771 [==============================] - 1s 809us/step - loss: 1.0869 - val_loss: 1.0858\n",
      "Epoch 68/200\n",
      "771/771 [==============================] - 1s 821us/step - loss: 1.0868 - val_loss: 1.0844\n",
      "Epoch 69/200\n",
      "771/771 [==============================] - 1s 809us/step - loss: 1.0855 - val_loss: 1.0857\n",
      "Epoch 70/200\n",
      "771/771 [==============================] - 1s 847us/step - loss: 1.0881 - val_loss: 1.0881\n",
      "Epoch 71/200\n",
      "771/771 [==============================] - 1s 829us/step - loss: 1.0853 - val_loss: 1.0849\n",
      "Epoch 72/200\n",
      "771/771 [==============================] - 1s 824us/step - loss: 1.0848 - val_loss: 1.0863\n",
      "Epoch 73/200\n",
      "771/771 [==============================] - 1s 827us/step - loss: 1.0846 - val_loss: 1.0874\n",
      "Epoch 74/200\n",
      "771/771 [==============================] - 1s 807us/step - loss: 1.0853 - val_loss: 1.0813\n",
      "Epoch 75/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0855 - val_loss: 1.0822\n",
      "Epoch 76/200\n",
      "771/771 [==============================] - 1s 847us/step - loss: 1.0859 - val_loss: 1.0831\n",
      "Epoch 77/200\n",
      "771/771 [==============================] - 1s 798us/step - loss: 1.0854 - val_loss: 1.0890\n",
      "Epoch 78/200\n",
      "771/771 [==============================] - 1s 817us/step - loss: 1.0840 - val_loss: 1.0865\n",
      "Epoch 79/200\n",
      "771/771 [==============================] - 1s 818us/step - loss: 1.0839 - val_loss: 1.0834\n",
      "Epoch 80/200\n",
      "771/771 [==============================] - 1s 812us/step - loss: 1.0829 - val_loss: 1.0830\n",
      "Epoch 81/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0827 - val_loss: 1.0835\n",
      "Epoch 82/200\n",
      "771/771 [==============================] - 1s 817us/step - loss: 1.0839 - val_loss: 1.0882\n",
      "Epoch 83/200\n",
      "771/771 [==============================] - 1s 864us/step - loss: 1.0828 - val_loss: 1.0802\n",
      "Epoch 84/200\n",
      "771/771 [==============================] - 1s 803us/step - loss: 1.0830 - val_loss: 1.0840\n",
      "Epoch 85/200\n",
      "771/771 [==============================] - 1s 815us/step - loss: 1.0825 - val_loss: 1.0812\n",
      "Epoch 86/200\n",
      "771/771 [==============================] - 1s 826us/step - loss: 1.0823 - val_loss: 1.0793\n",
      "Epoch 87/200\n",
      "771/771 [==============================] - 1s 825us/step - loss: 1.0819 - val_loss: 1.0782\n",
      "Epoch 88/200\n",
      "771/771 [==============================] - 1s 822us/step - loss: 1.0827 - val_loss: 1.0819\n",
      "Epoch 89/200\n",
      "771/771 [==============================] - 1s 800us/step - loss: 1.0811 - val_loss: 1.0806\n",
      "Epoch 90/200\n",
      "771/771 [==============================] - 1s 816us/step - loss: 1.0816 - val_loss: 1.0980\n",
      "Epoch 91/200\n",
      "771/771 [==============================] - 1s 818us/step - loss: 1.0821 - val_loss: 1.0819\n",
      "Epoch 92/200\n",
      "771/771 [==============================] - 1s 817us/step - loss: 1.0806 - val_loss: 1.0805\n",
      "Epoch 93/200\n",
      "771/771 [==============================] - 1s 795us/step - loss: 1.0812 - val_loss: 1.0760\n",
      "Epoch 94/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0808 - val_loss: 1.0856\n",
      "Epoch 95/200\n",
      "771/771 [==============================] - 1s 807us/step - loss: 1.0817 - val_loss: 1.0813\n",
      "Epoch 96/200\n",
      "771/771 [==============================] - 1s 820us/step - loss: 1.0807 - val_loss: 1.0803\n",
      "Epoch 97/200\n",
      "771/771 [==============================] - 1s 890us/step - loss: 1.0795 - val_loss: 1.0865\n",
      "Epoch 98/200\n",
      "771/771 [==============================] - 1s 823us/step - loss: 1.0790 - val_loss: 1.0820\n",
      "Epoch 99/200\n",
      "771/771 [==============================] - 1s 792us/step - loss: 1.0804 - val_loss: 1.0760\n",
      "Epoch 100/200\n",
      "771/771 [==============================] - 1s 819us/step - loss: 1.0788 - val_loss: 1.0833\n",
      "Epoch 101/200\n",
      "771/771 [==============================] - 1s 822us/step - loss: 1.0781 - val_loss: 1.0801\n",
      "Epoch 102/200\n",
      "771/771 [==============================] - 1s 808us/step - loss: 1.0782 - val_loss: 1.0795\n",
      "Epoch 103/200\n",
      "771/771 [==============================] - 1s 804us/step - loss: 1.0782 - val_loss: 1.0751\n",
      "Epoch 104/200\n",
      "771/771 [==============================] - 1s 815us/step - loss: 1.0785 - val_loss: 1.0789\n",
      "Epoch 105/200\n",
      "771/771 [==============================] - 1s 801us/step - loss: 1.0786 - val_loss: 1.0790\n",
      "Epoch 106/200\n",
      "771/771 [==============================] - 1s 829us/step - loss: 1.0792 - val_loss: 1.0797\n",
      "Epoch 107/200\n",
      "771/771 [==============================] - 1s 826us/step - loss: 1.0766 - val_loss: 1.0857\n",
      "Epoch 108/200\n",
      "771/771 [==============================] - 1s 794us/step - loss: 1.0773 - val_loss: 1.0796\n",
      "Epoch 109/200\n",
      "771/771 [==============================] - 1s 796us/step - loss: 1.0784 - val_loss: 1.0852\n",
      "Epoch 110/200\n",
      "771/771 [==============================] - 1s 834us/step - loss: 1.0780 - val_loss: 1.0769\n",
      "Epoch 111/200\n",
      "771/771 [==============================] - 1s 853us/step - loss: 1.0772 - val_loss: 1.0772\n",
      "Epoch 112/200\n",
      "771/771 [==============================] - 1s 813us/step - loss: 1.0777 - val_loss: 1.0771\n",
      "Epoch 113/200\n",
      "771/771 [==============================] - 1s 814us/step - loss: 1.0769 - val_loss: 1.0796\n",
      "Epoch 114/200\n",
      "771/771 [==============================] - 1s 816us/step - loss: 1.0766 - val_loss: 1.0769\n",
      "Epoch 115/200\n",
      "771/771 [==============================] - 1s 799us/step - loss: 1.0800 - val_loss: 1.0729\n",
      "Epoch 116/200\n",
      "771/771 [==============================] - 1s 803us/step - loss: 1.0774 - val_loss: 1.0813\n",
      "Epoch 117/200\n",
      "771/771 [==============================] - 1s 817us/step - loss: 1.0756 - val_loss: 1.0795\n",
      "Epoch 118/200\n",
      "771/771 [==============================] - 1s 819us/step - loss: 1.0763 - val_loss: 1.0746\n",
      "Epoch 119/200\n",
      "771/771 [==============================] - 1s 813us/step - loss: 1.0767 - val_loss: 1.0773\n",
      "Epoch 120/200\n",
      "771/771 [==============================] - 1s 802us/step - loss: 1.0754 - val_loss: 1.0739\n",
      "Epoch 121/200\n",
      "771/771 [==============================] - 1s 834us/step - loss: 1.0749 - val_loss: 1.0762\n",
      "Epoch 122/200\n",
      "771/771 [==============================] - 1s 820us/step - loss: 1.0767 - val_loss: 1.0754\n",
      "Epoch 123/200\n",
      "771/771 [==============================] - 1s 851us/step - loss: 1.0755 - val_loss: 1.0746\n",
      "Epoch 124/200\n",
      "771/771 [==============================] - 1s 815us/step - loss: 1.0755 - val_loss: 1.0775\n",
      "Epoch 125/200\n",
      "771/771 [==============================] - 1s 837us/step - loss: 1.0748 - val_loss: 1.0758\n",
      "Epoch 126/200\n",
      "771/771 [==============================] - 1s 818us/step - loss: 1.0761 - val_loss: 1.0746\n",
      "Epoch 127/200\n",
      "771/771 [==============================] - 1s 795us/step - loss: 1.0757 - val_loss: 1.0775\n",
      "Epoch 128/200\n",
      "771/771 [==============================] - 1s 806us/step - loss: 1.0755 - val_loss: 1.0727\n",
      "Epoch 129/200\n",
      "771/771 [==============================] - 1s 803us/step - loss: 1.0737 - val_loss: 1.0775\n",
      "Epoch 130/200\n",
      "771/771 [==============================] - 1s 820us/step - loss: 1.0753 - val_loss: 1.0843\n",
      "Epoch 131/200\n",
      "771/771 [==============================] - 1s 825us/step - loss: 1.0753 - val_loss: 1.0741\n",
      "Epoch 132/200\n",
      "771/771 [==============================] - 1s 797us/step - loss: 1.0740 - val_loss: 1.0711\n",
      "Epoch 133/200\n",
      "771/771 [==============================] - 1s 821us/step - loss: 1.0738 - val_loss: 1.0734\n",
      "Epoch 134/200\n",
      "771/771 [==============================] - 1s 792us/step - loss: 1.0733 - val_loss: 1.0727\n",
      "Epoch 135/200\n",
      "771/771 [==============================] - 1s 827us/step - loss: 1.0744 - val_loss: 1.0775\n",
      "Epoch 136/200\n",
      "771/771 [==============================] - 1s 872us/step - loss: 1.0743 - val_loss: 1.0758\n",
      "Epoch 137/200\n",
      "771/771 [==============================] - 1s 816us/step - loss: 1.0729 - val_loss: 1.0791\n",
      "Epoch 138/200\n",
      "771/771 [==============================] - 1s 804us/step - loss: 1.0747 - val_loss: 1.0871\n",
      "Epoch 139/200\n",
      "771/771 [==============================] - 1s 825us/step - loss: 1.0740 - val_loss: 1.0752\n",
      "Epoch 140/200\n",
      "771/771 [==============================] - 1s 874us/step - loss: 1.0735 - val_loss: 1.0739\n",
      "Epoch 141/200\n",
      "771/771 [==============================] - 1s 880us/step - loss: 1.0740 - val_loss: 1.0745\n",
      "Epoch 142/200\n",
      "771/771 [==============================] - 1s 887us/step - loss: 1.0729 - val_loss: 1.0767\n",
      "Epoch 143/200\n",
      "771/771 [==============================] - 1s 920us/step - loss: 1.0728 - val_loss: 1.0813\n",
      "Epoch 144/200\n",
      "771/771 [==============================] - 1s 934us/step - loss: 1.0727 - val_loss: 1.0741\n",
      "Epoch 145/200\n",
      "771/771 [==============================] - 1s 909us/step - loss: 1.0738 - val_loss: 1.0767\n",
      "Epoch 146/200\n",
      "771/771 [==============================] - 1s 878us/step - loss: 1.0722 - val_loss: 1.0778\n",
      "Epoch 147/200\n",
      "771/771 [==============================] - 1s 870us/step - loss: 1.0729 - val_loss: 1.0731\n",
      "Epoch 148/200\n",
      "771/771 [==============================] - 1s 896us/step - loss: 1.0723 - val_loss: 1.0728\n",
      "Epoch 149/200\n",
      "771/771 [==============================] - 1s 888us/step - loss: 1.0712 - val_loss: 1.0702\n",
      "Epoch 150/200\n",
      "771/771 [==============================] - 1s 974us/step - loss: 1.0723 - val_loss: 1.0771\n",
      "Epoch 151/200\n",
      "771/771 [==============================] - 1s 895us/step - loss: 1.0712 - val_loss: 1.0740\n",
      "Epoch 152/200\n",
      "771/771 [==============================] - 1s 884us/step - loss: 1.0726 - val_loss: 1.0693\n",
      "Epoch 153/200\n",
      "771/771 [==============================] - 1s 895us/step - loss: 1.0724 - val_loss: 1.0706\n",
      "Epoch 154/200\n",
      "771/771 [==============================] - 1s 891us/step - loss: 1.0717 - val_loss: 1.0742\n",
      "Epoch 155/200\n",
      "771/771 [==============================] - 1s 895us/step - loss: 1.0709 - val_loss: 1.0740\n",
      "Epoch 156/200\n",
      "771/771 [==============================] - 1s 879us/step - loss: 1.0706 - val_loss: 1.0673\n",
      "Epoch 157/200\n",
      "771/771 [==============================] - 1s 904us/step - loss: 1.0703 - val_loss: 1.0716\n",
      "Epoch 158/200\n",
      "771/771 [==============================] - 1s 908us/step - loss: 1.0705 - val_loss: 1.0834\n",
      "Epoch 159/200\n",
      "771/771 [==============================] - 1s 928us/step - loss: 1.0703 - val_loss: 1.0713\n",
      "Epoch 160/200\n",
      "771/771 [==============================] - 1s 876us/step - loss: 1.0697 - val_loss: 1.0732\n",
      "Epoch 161/200\n",
      "771/771 [==============================] - 1s 892us/step - loss: 1.0701 - val_loss: 1.0815\n",
      "Epoch 162/200\n",
      "771/771 [==============================] - 1s 975us/step - loss: 1.0701 - val_loss: 1.0740\n",
      "Epoch 163/200\n",
      "771/771 [==============================] - 1s 899us/step - loss: 1.0709 - val_loss: 1.0690\n",
      "Epoch 164/200\n",
      "771/771 [==============================] - 1s 873us/step - loss: 1.0704 - val_loss: 1.0790\n",
      "Epoch 165/200\n",
      "771/771 [==============================] - 1s 887us/step - loss: 1.0700 - val_loss: 1.0680\n",
      "Epoch 166/200\n",
      "771/771 [==============================] - 1s 905us/step - loss: 1.0694 - val_loss: 1.0674\n",
      "Epoch 167/200\n",
      "771/771 [==============================] - 1s 889us/step - loss: 1.0698 - val_loss: 1.0782\n",
      "Epoch 168/200\n",
      "771/771 [==============================] - 1s 905us/step - loss: 1.0707 - val_loss: 1.0783\n",
      "Epoch 169/200\n",
      "771/771 [==============================] - 1s 885us/step - loss: 1.0698 - val_loss: 1.0702\n",
      "Epoch 170/200\n",
      "771/771 [==============================] - 1s 879us/step - loss: 1.0679 - val_loss: 1.0679\n",
      "Epoch 171/200\n",
      "771/771 [==============================] - 1s 900us/step - loss: 1.0686 - val_loss: 1.0710\n",
      "Epoch 172/200\n",
      "771/771 [==============================] - 1s 877us/step - loss: 1.0686 - val_loss: 1.0706\n",
      "Epoch 173/200\n",
      "771/771 [==============================] - 1s 911us/step - loss: 1.0685 - val_loss: 1.0701\n",
      "Epoch 174/200\n",
      "771/771 [==============================] - 1s 883us/step - loss: 1.0681 - val_loss: 1.0704\n",
      "Epoch 175/200\n",
      "771/771 [==============================] - 1s 1ms/step - loss: 1.0692 - val_loss: 1.0675\n",
      "Epoch 176/200\n",
      "771/771 [==============================] - 1s 897us/step - loss: 1.0688 - val_loss: 1.0708\n",
      "Epoch 177/200\n",
      "771/771 [==============================] - 1s 876us/step - loss: 1.0682 - val_loss: 1.0674\n",
      "Epoch 178/200\n",
      "771/771 [==============================] - 1s 886us/step - loss: 1.0681 - val_loss: 1.0684\n",
      "Epoch 179/200\n",
      "771/771 [==============================] - 1s 877us/step - loss: 1.0682 - val_loss: 1.0699\n",
      "Epoch 180/200\n",
      "771/771 [==============================] - 1s 883us/step - loss: 1.0680 - val_loss: 1.0737\n",
      "Epoch 181/200\n",
      "771/771 [==============================] - 1s 893us/step - loss: 1.0674 - val_loss: 1.0648\n",
      "Epoch 182/200\n",
      "771/771 [==============================] - 1s 898us/step - loss: 1.0676 - val_loss: 1.0705\n",
      "Epoch 183/200\n",
      "771/771 [==============================] - 1s 896us/step - loss: 1.0673 - val_loss: 1.0678\n",
      "Epoch 184/200\n",
      "771/771 [==============================] - 1s 878us/step - loss: 1.0672 - val_loss: 1.0763\n",
      "Epoch 185/200\n",
      "771/771 [==============================] - 1s 886us/step - loss: 1.0680 - val_loss: 1.0782\n",
      "Epoch 186/200\n",
      "771/771 [==============================] - 1s 874us/step - loss: 1.0680 - val_loss: 1.0747\n",
      "Epoch 187/200\n",
      "771/771 [==============================] - 1s 886us/step - loss: 1.0662 - val_loss: 1.0800\n",
      "Epoch 188/200\n",
      "771/771 [==============================] - 1s 873us/step - loss: 1.0681 - val_loss: 1.0680\n",
      "Epoch 189/200\n",
      "771/771 [==============================] - 1s 975us/step - loss: 1.0672 - val_loss: 1.0679\n",
      "Epoch 190/200\n",
      "771/771 [==============================] - 1s 894us/step - loss: 1.0661 - val_loss: 1.0804\n",
      "Epoch 191/200\n",
      "771/771 [==============================] - 1s 883us/step - loss: 1.0655 - val_loss: 1.0756\n",
      "Epoch 192/200\n",
      "771/771 [==============================] - 1s 882us/step - loss: 1.0654 - val_loss: 1.0752\n",
      "Epoch 193/200\n",
      "771/771 [==============================] - 1s 873us/step - loss: 1.0670 - val_loss: 1.0780\n",
      "Epoch 194/200\n",
      "771/771 [==============================] - 1s 891us/step - loss: 1.0653 - val_loss: 1.0732\n",
      "Epoch 195/200\n",
      "771/771 [==============================] - 1s 883us/step - loss: 1.0676 - val_loss: 1.0794\n",
      "Epoch 196/200\n",
      "771/771 [==============================] - 1s 887us/step - loss: 1.0663 - val_loss: 1.0707\n",
      "Epoch 197/200\n",
      "771/771 [==============================] - 1s 887us/step - loss: 1.0656 - val_loss: 1.0738\n",
      "Epoch 198/200\n",
      "771/771 [==============================] - 1s 881us/step - loss: 1.0666 - val_loss: 1.0748\n",
      "Epoch 199/200\n",
      "771/771 [==============================] - 1s 894us/step - loss: 1.0651 - val_loss: 1.0779\n",
      "Epoch 200/200\n",
      "771/771 [==============================] - 1s 881us/step - loss: 1.0651 - val_loss: 1.0703\n"
     ]
    }
   ],
   "source": [
    "ann_r = ann.fit(X_train, y_train, validation_split=0.3, epochs=200, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :->  0.7596266918177867\n",
      "Mean Squared Error :->  1.0591658087561415\n",
      "Root Mean Squared Error :->  1.0291578152820595\n",
      "R-Square :->  0.7790035758287919\n"
     ]
    }
   ],
   "source": [
    "y_pred_ann = ann.predict(X_test)\n",
    "mae, mse, rmse, r2 = evaluate(y_test, y_pred_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_score['ANN'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(x=list(algo_score.keys()), y=list(algo_score.values()))\n",
    "plt.title(\"Algorithms Performance\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.ylabel(\"R-Squared Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ---> 0.8104701460640636\n",
      "K Nearest Neighbour ---> 0.7538540204577808\n",
      "Linear Regression ---> 0.6358846067618822\n",
      "Decision Tree ---> 0.6750111802932943\n",
      "XG Boost ---> 0.8101861170775692\n",
      "ANN ---> 0.7790035758287919\n",
      "Optimized XG Boost ---> 0.8140431669907069\n"
     ]
    }
   ],
   "source": [
    "for key, val in algo_score.items():\n",
    "    print(key, '--->', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best performing Model is Optimized XG Boost"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ddfba760c93d0781cb88c4db9a31eb68ca4e1616821530f19d735f356a5c9ec"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
